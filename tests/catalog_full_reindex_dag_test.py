"""Unit Tests for the TUL Cob Catalog Full Reindex DAG."""
import os
import unittest
from unittest.mock import patch
import requests
import requests_mock
import airflow
from cob_datapipeline.catalog_full_reindex_dag import DAG,\
        CATALOG_PRE_PRODUCTION_HARVEST_FROM_DATE,\
        index_sftp_marc_group
        
from tests.helpers import get_connection

class TestCatalogFullReindexDag(unittest.TestCase):
    """Unit Tests for solrcloud catalog full reindex dag file."""

    def setUp(self):
        """Method to set up the DAG Class instance for testing."""
        self.tasks = list(map(lambda t: t.task_id, DAG.tasks))

    def test_dag_loads(self):
        """Unit test that the DAG identifier is set correctly."""
        self.assertEqual(DAG.dag_id, "catalog_full_reindex")


    def test_dag_tasks_present(self):
        """Unit test that the DAG instance contains the expected tasks."""
        self.assertEqual(self.tasks, [
            "safety_check",
            "verify_prod_collection",
            "set_s3_namespace",
            "list_alma_s3_data",
            "list_boundwith_s3_data",
            "prepare_boundwiths",
            "prepare_alma_data",
            "create_collection",
            "push_collection",
            "delete_collections",
            "get_num_solr_docs_pre",
            "list_s3_marc_files",
            "index_sftp_marc",
            "solr_commit",
            "update_variables",
            "get_num_solr_docs_post",
            "slack_success_post",
            ])

    def test_dag_task_order(self):
        """Unit test that the DAG instance contains the expected dependencies."""
        expected_task_deps = {
            "set_s3_namespace": ["safety_check", "verify_prod_collection"],
            "list_alma_s3_data": ["set_s3_namespace"],
            "list_boundwith_s3_data": ["set_s3_namespace"],
            "prepare_boundwiths": ["list_boundwith_s3_data"],
            "prepare_alma_data": ["list_alma_s3_data", "prepare_boundwiths"],
            "create_collection": ["prepare_alma_data", "prepare_boundwiths"],
            "push_collection": ["create_collection"],
            "delete_collections": ["push_collection"],
            "get_num_solr_docs_pre": ["delete_collections"],
            "list_s3_marc_files": ["get_num_solr_docs_pre"],
            "index_sftp_marc": ["list_s3_marc_files"],
            "solr_commit": ["index_sftp_marc"],
            "update_variables": ["solr_commit"],
            "get_num_solr_docs_post": ["update_variables"],
            "slack_success_post": ["get_num_solr_docs_post"],
        }
        for task, upstream_tasks in expected_task_deps.items():
            upstream_list = [up_task.task_id for up_task in DAG.get_task(task).upstream_list]
            self.assertCountEqual(upstream_tasks, upstream_list)

    @requests_mock.mock()
    def test_verify_prod_collection_task(self, mock_request):

        mock_request.get(
            'https://testhost/okcomputer/solr/foo',
            status_code=200,
            text='OK')

        with DAG, patch('airflow.hooks.base.BaseHook.get_connection',
                   side_effect=get_connection):

            DAG.get_task("verify_prod_collection").execute(None)


    @requests_mock.mock()
    def test_verify_prod_collection_task_with_fail(self, mock_request):

        mock_request.get(
            'https://testhost/okcomputer/solr/foo',
            status_code=400,
            reason='Not Found',
            text='Boo')

        with DAG, patch('airflow.hooks.base.BaseHook.get_connection',
                side_effect=get_connection), self.assertRaises(airflow.exceptions.AirflowException):

            DAG.get_task("verify_prod_collection").execute(None)

    def test_we_calculate_correct_harvest_from_date(self):
        self.assertEqual(CATALOG_PRE_PRODUCTION_HARVEST_FROM_DATE, "2020-06-07T00:00:00Z" )

class TestIndexSftpMarcGroup(unittest.TestCase):
    def setUp(self):
        # Create a DAG for testing
        self.dag = airflow.DAG(
            dag_id="test_dag",
            default_args={"owner": "airflow", "retries": 1},
        )
    def test_index_sftp_marc_group_tasks(self):
        # Mock file list to use in tests
        mock_file_list = ["file1.xml", "file2.xml", "file3.xml", "file4.xml", "file5.xml", "file6.xml"]

        # Use the file_processing_group within the DAG context
        with self.dag:
            task_group = index_sftp_marc_group(mock_file_list)

        # Access the tasks generated by the task group
        tasks_in_group = task_group.children

        # Ensure the number of tasks matches expected (2 chunks in this case)
        self.assertEqual(len(tasks_in_group), 2)

        # Check if the correct task IDs are generated
        expected_task_ids = ["index_sftp_marc_group.index_sftp_marc_0", "index_sftp_marc_group.index_sftp_marc_1"]
        for expected_id in expected_task_ids:
            self.assertIn(expected_id, tasks_in_group)

        # Optionally print the task IDs for debugging
        for task_id, task in tasks_in_group.items():
            print(f"Task ID: {task_id}, Task: {task}")

        # """Test that we index files"""
        airflow_home = airflow.models.Variable.get("AIRFLOW_HOME")
        task = self.dag.get_task("index_sftp_marc_group.index_sftp_marc_0")
        expected_bash_path = airflow_home + "/dags/cob_datapipeline/scripts/ingest_marc.sh "
        self.assertEqual(task.env["HOME"], os.getcwd())
        self.assertEqual(task.bash_command, expected_bash_path)

